{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ocefpaf/inundation_notebook/blob/master/inundation_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "\n",
    "# Choose the date range: 7 days from today.\n",
    "run_name = date.today().strftime(\"%Y-%m-%d\")\n",
    "today = datetime.strptime(run_name, \"%Y-%m-%d\")\n",
    "today = today.replace(tzinfo=pytz.utc)\n",
    "\n",
    "start = today - timedelta(days=4)\n",
    "stop = today + timedelta(days=3)\n",
    "\n",
    "# NERACOOS Mass Bay Region.\n",
    "bbox = [-72, 41, -69, 44]\n",
    "\n",
    "# CF-names to look for (Sea Surface Height).\n",
    "name_list = ['sea_surface_height',\n",
    "             'sea_surface_elevation',\n",
    "             'sea_surface_height_above_geoid',\n",
    "             'sea_surface_height_above_sea_level',\n",
    "             'water_surface_height_above_reference_datum',\n",
    "             'sea_surface_height_above_reference_ellipsoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "import pyoos\n",
    "import owslib\n",
    "\n",
    "import logging as log\n",
    "reload(log)\n",
    "\n",
    "fmt = '{:*^64}'.format\n",
    "log.captureWarnings(True)\n",
    "LOG_FILENAME = 'log.txt'\n",
    "log.basicConfig(filename=LOG_FILENAME,\n",
    "                filemode='w',\n",
    "                format='%(asctime)s %(levelname)s: %(message)s',\n",
    "                datefmt='%I:%M:%S',\n",
    "                level=log.INFO,\n",
    "                stream=None)\n",
    "\n",
    "log.info(fmt(' Run information '))\n",
    "log.info('Run date: {:%Y-%m-%d %H:%M:%S}'.format(datetime.utcnow()))\n",
    "log.info('Download start: {:%Y-%m-%d %H:%M:%S}'.format(start))\n",
    "log.info('Download stop: {:%Y-%m-%d %H:%M:%S}'.format(stop))\n",
    "log.info('Bounding box: {0:3.2f}, {1:3.2f},'\n",
    "         '{2:3.2f}, {3:3.2f}'.format(*bbox))\n",
    "log.info(fmt(' Software version '))\n",
    "log.info('Iris version: {}'.format(iris.__version__))\n",
    "log.info('owslib version: {}'.format(owslib.__version__))\n",
    "log.info('pyoos version: {}'.format(pyoos.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fes_date_filter(start, stop, constraint='overlaps'):\n",
    "    \"\"\"Take datetime-like objects and returns a fes filter for date range.\n",
    "    NOTE: Truncates the minutes!\"\"\"\n",
    "    start = start.strftime('%Y-%m-%d %H:00')\n",
    "    stop = stop.strftime('%Y-%m-%d %H:00')\n",
    "    if constraint == 'overlaps':\n",
    "        propertyname = 'apiso:TempExtent_begin'\n",
    "        begin = fes.PropertyIsLessThanOrEqualTo(propertyname=propertyname,\n",
    "                                                literal=stop)\n",
    "        propertyname = 'apiso:TempExtent_end'\n",
    "        end = fes.PropertyIsGreaterThanOrEqualTo(propertyname=propertyname,\n",
    "                                                 literal=start)\n",
    "    elif constraint == 'within':\n",
    "        propertyname = 'apiso:TempExtent_begin'\n",
    "        begin = fes.PropertyIsGreaterThanOrEqualTo(propertyname=propertyname,\n",
    "                                                   literal=start)\n",
    "        propertyname = 'apiso:TempExtent_end'\n",
    "        end = fes.PropertyIsLessThanOrEqualTo(propertyname=propertyname,\n",
    "                                              literal=stop)\n",
    "    else:\n",
    "        raise NameError('Unrecognized constraint {}'.format(constraint))\n",
    "    return begin, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from owslib import fes\n",
    "\n",
    "kw = dict(wildCard='*',\n",
    "          escapeChar='\\\\',\n",
    "          singleChar='?',\n",
    "          propertyname='apiso:AnyText')\n",
    "\n",
    "or_filt = fes.Or([fes.PropertyIsLike(literal=('*%s*' % val), **kw)\n",
    "                  for val in name_list])\n",
    "\n",
    "# Exculde ROMS Averages and History files.\n",
    "not_filt = fes.Not([fes.PropertyIsLike(literal='*Averages*', **kw)])\n",
    "\n",
    "begin, end = fes_date_filter(start, stop)\n",
    "filter_list = [fes.And([fes.BBox(bbox), begin, end, or_filt, not_filt])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from owslib.csw import CatalogueServiceWeb\n",
    "\n",
    "endpoint = 'http://www.ngdc.noaa.gov/geoportal/csw'\n",
    "csw = CatalogueServiceWeb(endpoint, timeout=60)\n",
    "csw.getrecords2(constraints=filter_list, maxrecords=1000, esn='full')\n",
    "\n",
    "log.info(fmt(' Catalog information '))\n",
    "log.info(\"URL: {}\".format(endpoint))\n",
    "log.info(\"CSW version: {}\".format(csw.version))\n",
    "log.info(\"Number of datasets available: {}\".format(len(csw.records.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def service_urls(records, service='odp:url'):\n",
    "    \"\"\"Extract service_urls of a specific type (DAP, SOS) from records.\"\"\"\n",
    "    service_string = 'urn:x-esri:specification:ServiceType:' + service\n",
    "    urls = []\n",
    "    for key, rec in records.items():\n",
    "        # Create a generator object, and iterate through it until the match is\n",
    "        # found if not found, gets the default value (here \"none\").\n",
    "        url = next((d['url'] for d in rec.references if\n",
    "                    d['scheme'] == service_string), None)\n",
    "        if url is not None:\n",
    "            urls.append(url)\n",
    "    urls = sorted(set(urls))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dap_urls = service_urls(csw.records, service='odp:url')\n",
    "sos_urls = service_urls(csw.records, service='sos:url')\n",
    "\n",
    "log.info(fmt(' CSW '))\n",
    "for rec, item in csw.records.items():\n",
    "    log.info('{}'.format(item.title))\n",
    "\n",
    "log.info(fmt(' DAP '))\n",
    "for url in dap_urls:\n",
    "    log.info('{}.html'.format(url))\n",
    "\n",
    "log.info(fmt(' SOS '))\n",
    "for url in sos_urls:\n",
    "    log.info('{}'.format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyoos.collectors.coops.coops_sos import CoopsSos\n",
    "\n",
    "collector = CoopsSos()\n",
    "sos_name = 'water_surface_height_above_reference_datum'\n",
    "\n",
    "datum = 'NAVD'\n",
    "collector.set_datum(datum)\n",
    "collector.end_time = stop\n",
    "collector.start_time = start\n",
    "collector.variables = [sos_name]\n",
    "\n",
    "ofrs = collector.server.offerings\n",
    "title = collector.server.identification.title\n",
    "log.info(fmt(' Collector offerings '))\n",
    "log.info('{}: {} offerings'.format(title, len(ofrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urlparse import urlparse\n",
    "\n",
    "\n",
    "# Web-parsing.\n",
    "def parse_url(url):\n",
    "    \"\"\"This will preserve any given scheme but will add http if none is\n",
    "    provided.\"\"\"\n",
    "    if not urlparse(url).scheme:\n",
    "        url = \"http://{}\".format(url)\n",
    "    return url\n",
    "\n",
    "\n",
    "def sos_request(url='opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS', **kw):\n",
    "    url = parse_url(url)\n",
    "    offering = 'urn:ioos:network:NOAA.NOS.CO-OPS:CurrentsActive'\n",
    "    params = dict(service='SOS',\n",
    "                  request='GetObservation',\n",
    "                  version='1.0.0',\n",
    "                  offering=offering,\n",
    "                  responseFormat='text/csv')\n",
    "    params.update(kw)\n",
    "    r = requests.get(url, params=params)\n",
    "    r.raise_for_status()\n",
    "    content = r.headers['Content-Type']\n",
    "    if 'excel' in content or 'csv' in content:\n",
    "        return r.url\n",
    "    else:\n",
    "        raise TypeError('Bad url {}'.format(r.url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "params = dict(observedProperty=sos_name,\n",
    "              eventTime=start.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "              featureOfInterest='BBOX:{0},{1},{2},{3}'.format(*bbox),\n",
    "              offering='urn:ioos:network:NOAA.NOS.CO-OPS:WaterLevelActive')\n",
    "\n",
    "uri = 'http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS'\n",
    "url = sos_request(uri, **params)\n",
    "observations = read_csv(url)\n",
    "\n",
    "log.info('SOS URL request: {}'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the dataframe (visualization purpose only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from urllib import urlopen\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def get_coops_longname(station):\n",
    "    \"\"\"Get longName for specific station from COOPS SOS using DescribeSensor\n",
    "    request.\"\"\"\n",
    "    url = ('opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&'\n",
    "           'request=DescribeSensor&version=1.0.0&'\n",
    "           'outputFormat=text/xml;subtype=\"sensorML/1.0.1\"&'\n",
    "           'procedure=urn:ioos:station:NOAA.NOS.CO-OPS:%s') % station\n",
    "    url = parse_url(url)\n",
    "    tree = etree.parse(urlopen(url))\n",
    "    root = tree.getroot()\n",
    "    path = \"//sml:identifier[@name='longName']/sml:Term/sml:value/text()\"\n",
    "    namespaces = dict(sml=\"http://www.opengis.net/sensorML/1.0.1\")\n",
    "    longName = root.xpath(path, namespaces=namespaces)\n",
    "    if len(longName) == 0:\n",
    "        longName = station\n",
    "    return longName[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>sensor</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>date_time</th>\n",
       "      <th>ssh above datum</th>\n",
       "      <th>datum</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Portland, ME</th>\n",
       "      <td> 8418150</td>\n",
       "      <td> A1</td>\n",
       "      <td> 43.6567</td>\n",
       "      <td>-70.2467</td>\n",
       "      <td> 2015-01-24T00:00:00Z</td>\n",
       "      <td>-0.495</td>\n",
       "      <td> MLLW</td>\n",
       "      <td> 2.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wells, ME</th>\n",
       "      <td> 8419317</td>\n",
       "      <td> B1</td>\n",
       "      <td> 43.3200</td>\n",
       "      <td>-70.5633</td>\n",
       "      <td> 2015-01-24T00:00:00Z</td>\n",
       "      <td>-0.519</td>\n",
       "      <td> MLLW</td>\n",
       "      <td> 4.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fort Point, NH</th>\n",
       "      <td> 8423898</td>\n",
       "      <td> A1</td>\n",
       "      <td> 43.0717</td>\n",
       "      <td>-70.7117</td>\n",
       "      <td> 2015-01-24T00:00:00Z</td>\n",
       "      <td>-0.479</td>\n",
       "      <td> MLLW</td>\n",
       "      <td> 0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston, MA</th>\n",
       "      <td> 8443970</td>\n",
       "      <td> B1</td>\n",
       "      <td> 42.3548</td>\n",
       "      <td>-71.0534</td>\n",
       "      <td> 2015-01-24T00:00:00Z</td>\n",
       "      <td>-0.492</td>\n",
       "      <td> MLLW</td>\n",
       "      <td> 1.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fall River, MA</th>\n",
       "      <td> 8447386</td>\n",
       "      <td> B1</td>\n",
       "      <td> 41.7043</td>\n",
       "      <td>-71.1641</td>\n",
       "      <td> 2015-01-24T00:00:00Z</td>\n",
       "      <td> 0.592</td>\n",
       "      <td> MLLW</td>\n",
       "      <td> 6.356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                station sensor      lat      lon             date_time  \\\n",
       "name                                                                     \n",
       "Portland, ME    8418150     A1  43.6567 -70.2467  2015-01-24T00:00:00Z   \n",
       "Wells, ME       8419317     B1  43.3200 -70.5633  2015-01-24T00:00:00Z   \n",
       "Fort Point, NH  8423898     A1  43.0717 -70.7117  2015-01-24T00:00:00Z   \n",
       "Boston, MA      8443970     B1  42.3548 -71.0534  2015-01-24T00:00:00Z   \n",
       "Fall River, MA  8447386     B1  41.7043 -71.1641  2015-01-24T00:00:00Z   \n",
       "\n",
       "                ssh above datum datum  height  \n",
       "name                                           \n",
       "Portland, ME             -0.495  MLLW   2.607  \n",
       "Wells, ME                -0.519  MLLW   4.500  \n",
       "Fort Point, NH           -0.479  MLLW   0.829  \n",
       "Boston, MA               -0.492  MLLW   1.074  \n",
       "Fall River, MA            0.592  MLLW   6.356  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = {'datum_id': 'datum',\n",
    "           'sensor_id': 'sensor',\n",
    "           'station_id': 'station',\n",
    "           'latitude (degree)': 'lat',\n",
    "           'longitude (degree)': 'lon',\n",
    "           'vertical_position (m)': 'height',\n",
    "           'water_surface_height_above_reference_datum (m)': 'ssh above datum'}\n",
    "\n",
    "observations.rename(columns=columns, inplace=True)\n",
    "\n",
    "observations['datum'] = [s.split(':')[-1] for s in observations['datum']]\n",
    "observations['sensor'] = [s.split(':')[-1] for s in observations['sensor']]\n",
    "observations['station'] = [s.split(':')[-1] for s in observations['station']]\n",
    "observations['name'] = [get_coops_longname(s) for s in observations['station']]\n",
    "\n",
    "observations.set_index('name', inplace=True)\n",
    "\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a uniform 6-min time base for model/data comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from iris.pandas import as_cube\n",
    "\n",
    "\n",
    "def coops2df(collector, coops_id):\n",
    "    \"\"\"Request CSV response from SOS and convert to Pandas DataFrames.\"\"\"\n",
    "    collector.features = [coops_id]\n",
    "    long_name = get_coops_longname(coops_id)\n",
    "    response = collector.raw(responseFormat=\"text/csv\")\n",
    "    kw = dict(parse_dates=True, index_col='date_time')\n",
    "    data_df = read_csv(BytesIO(response.encode('utf-8')), **kw)\n",
    "    data_df.name = long_name\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def save_timeseries(df, outfile, standard_name, **kw):\n",
    "    \"\"\"http://cfconventions.org/Data/cf-convetions/cf-conventions-1.6/build/cf-conventions.html#idp5577536\"\"\"\n",
    "    cube = as_cube(df, calendars={1: iris.unit.CALENDAR_GREGORIAN})\n",
    "    cube.coord(\"index\").rename(\"time\")\n",
    "    cube.coord(\"columns\").rename(\"station name\")\n",
    "    cube.rename(standard_name)\n",
    "\n",
    "    longitude = kw.get(\"longitude\")\n",
    "    latitude = kw.get(\"latitude\")\n",
    "    if longitude is not None:\n",
    "        longitude = iris.coords.AuxCoord(longitude,\n",
    "                                         var_name=\"lon\",\n",
    "                                         standard_name=\"longitude\",\n",
    "                                         long_name=\"station longitude\",\n",
    "                                         units=iris.unit.Unit(\"degrees\"))\n",
    "    cube.add_aux_coord(longitude, data_dims=1)\n",
    "\n",
    "    if latitude is not None:\n",
    "        latitude = iris.coords.AuxCoord(latitude,\n",
    "                                        var_name=\"lat\",\n",
    "                                        standard_name=\"latitude\",\n",
    "                                        long_name=\"station latitude\",\n",
    "                                        units=iris.unit.Unit(\"degrees\"))\n",
    "        cube.add_aux_coord(latitude, data_dims=1)\n",
    "\n",
    "    # Work around iris to get String instead of np.array object.\n",
    "    string_list = cube.coord(\"station name\").points.tolist()\n",
    "    cube.coord(\"station name\").points = string_list\n",
    "    cube.coord(\"station name\").var_name = 'station'\n",
    "\n",
    "    station_attr = kw.get(\"station_attr\")\n",
    "    if station_attr is not None:\n",
    "        cube.coord(\"station name\").attributes.update(station_attr)\n",
    "\n",
    "    cube_attr = kw.get(\"cube_attr\")\n",
    "    if cube_attr is not None:\n",
    "        cube.attributes.update(cube_attr)\n",
    "\n",
    "    iris.save(cube, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8418150</th>\n",
       "      <th>8423898</th>\n",
       "      <th>8443970</th>\n",
       "      <th>8447930</th>\n",
       "      <th>8452660</th>\n",
       "      <th>8454000</th>\n",
       "      <th>8510560</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-24 00:00:00</th>\n",
       "      <td>-2.096</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.170</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-24 00:06:00</th>\n",
       "      <td>-2.109</td>\n",
       "      <td>-2.007</td>\n",
       "      <td>-2.205</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>   NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-24 00:12:00</th>\n",
       "      <td>-2.119</td>\n",
       "      <td>-2.018</td>\n",
       "      <td>-2.234</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>   NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-24 00:18:00</th>\n",
       "      <td>-2.119</td>\n",
       "      <td>-2.029</td>\n",
       "      <td>-2.249</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-24 00:24:00</th>\n",
       "      <td>-2.110</td>\n",
       "      <td>-2.025</td>\n",
       "      <td>-2.264</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.022</td>\n",
       "      <td> 0.004</td>\n",
       "      <td>   NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     8418150  8423898  8443970  8447930  8452660  8454000  \\\n",
       "date_time                                                                   \n",
       "2015-01-24 00:00:00   -2.096   -2.000   -2.170   -0.296   -0.153   -0.167   \n",
       "2015-01-24 00:06:00   -2.109   -2.007   -2.205   -0.287   -0.121   -0.136   \n",
       "2015-01-24 00:12:00   -2.119   -2.018   -2.234   -0.273   -0.087   -0.096   \n",
       "2015-01-24 00:18:00   -2.119   -2.029   -2.249   -0.257   -0.055   -0.049   \n",
       "2015-01-24 00:24:00   -2.110   -2.025   -2.264   -0.237   -0.022    0.004   \n",
       "\n",
       "                     8510560  \n",
       "date_time                     \n",
       "2015-01-24 00:00:00   -0.342  \n",
       "2015-01-24 00:06:00      NaN  \n",
       "2015-01-24 00:12:00      NaN  \n",
       "2015-01-24 00:18:00   -0.273  \n",
       "2015-01-24 00:24:00      NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import iris\n",
    "from pandas import DataFrame\n",
    "from owslib.ows import ExceptionReport\n",
    "\n",
    "iris.FUTURE.netcdf_promote = True\n",
    "\n",
    "log.info(fmt(' Observations '))\n",
    "fname = '{}-OBS_DATA.nc'.format(run_name)\n",
    "\n",
    "log.info(fmt(' Downloading to file {} '.format(fname)))\n",
    "data = dict()\n",
    "bad_datum = []\n",
    "for station in observations.station:\n",
    "    try:\n",
    "        df = coops2df(collector, station)\n",
    "        col = 'water_surface_height_above_reference_datum (m)'\n",
    "        data.update({station: df[col]})\n",
    "    except ExceptionReport as e:\n",
    "        bad_datum.append(station)\n",
    "        name = get_coops_longname(station)\n",
    "        log.warning(\"[{}] {}:\\n{}\".format(station, name, e))\n",
    "obs_data = DataFrame.from_dict(data)\n",
    "\n",
    "# Split good and bad vertical datum stations.\n",
    "pattern = '|'.join(bad_datum)\n",
    "if pattern:\n",
    "    non_navd = observations.station.str.contains(pattern)\n",
    "    bad_datum = observations[non_navd]\n",
    "    observations = observations[~non_navd]\n",
    "\n",
    "comment = \"Several stations from http://opendap.co-ops.nos.noaa.gov\"\n",
    "kw = dict(longitude=observations.lon,\n",
    "          latitude=observations.lat,\n",
    "          station_attr=dict(cf_role=\"timeseries_id\"),\n",
    "          cube_attr=dict(featureType='timeSeries',\n",
    "                         Conventions='CF-1.6',\n",
    "                         standard_name_vocabulary='CF-1.6',\n",
    "                         cdm_data_type=\"Station\",\n",
    "                         comment=comment,\n",
    "                         datum=datum,\n",
    "                         url=url))\n",
    "\n",
    "save_timeseries(obs_data, outfile=fname,\n",
    "                standard_name=sos_name, **kw)\n",
    "\n",
    "obs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop discovered models and save the nearest time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "from oceans import wrap_lon180\n",
    "\n",
    "from iris import Constraint\n",
    "from iris.cube import CubeList\n",
    "from iris.exceptions import CoordinateMultiDimError, CoordinateNotFoundError\n",
    "\n",
    "water_level = ['sea_surface_height',\n",
    "               'sea_surface_elevation',\n",
    "               'sea_surface_height_above_geoid',\n",
    "               'sea_surface_height_above_sea_level',\n",
    "               'water_surface_height_above_reference_datum',\n",
    "               'sea_surface_height_above_reference_ellipsoid']\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    \"\"\"\n",
    "    Example\n",
    "    -------\n",
    "    >>> def long_function_call():\n",
    "    >>>     import time\n",
    "    >>>     sec = 0\n",
    "    >>>>    while True:\n",
    "    >>>         sec += 1\n",
    "    >>>         print(sec)\n",
    "    >>>         time.sleep(1)\n",
    "    >>>\n",
    "    >>> try:\n",
    "    >>>     with time_limit(10):\n",
    "    >>>     long_function_call()\n",
    "    >>> except TimeoutException as msg:\n",
    "    >>>     print(\"Timed out!\")\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds=10):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "# Iris.\n",
    "def z_coord(cube):\n",
    "    \"\"\"Heuristic way to return **one** the vertical coordinate.\"\"\"\n",
    "    try:\n",
    "        z = cube.coord(axis='Z')\n",
    "    except CoordinateNotFoundError:\n",
    "        z = None\n",
    "        for coord in cube.coords(axis='Z'):\n",
    "            if coord.name() not in water_level:\n",
    "                z = coord\n",
    "    return z\n",
    "\n",
    "\n",
    "def get_surface(cube):\n",
    "    \"\"\"Work around `iris.cube.Cube.slices` error:\n",
    "    The requested coordinates are not orthogonal.\"\"\"\n",
    "    z = z_coord(cube)\n",
    "    if z:\n",
    "        positive = z.attributes.get('positive', None)\n",
    "        if positive == 'up':\n",
    "            idx = np.unique(z.points.argmax(axis=0))[0]\n",
    "        else:\n",
    "            idx = np.unique(z.points.argmin(axis=0))[0]\n",
    "        return cube[:, idx, ...]\n",
    "    else:\n",
    "        return cube\n",
    "\n",
    "\n",
    "def time_coord(cube):\n",
    "    \"\"\"Return the variable attached to time axis and rename it to time.\"\"\"\n",
    "    try:\n",
    "        cube.coord(axis='T').rename('time')\n",
    "    except CoordinateNotFoundError:\n",
    "        pass\n",
    "    timevar = cube.coord('time')\n",
    "    return timevar\n",
    "\n",
    "\n",
    "def time_near(cube, datetime):\n",
    "    \"\"\"Return the nearest index to a `datetime`.\"\"\"\n",
    "    timevar = time_coord(cube)\n",
    "    try:\n",
    "        time = timevar.units.date2num(datetime)\n",
    "        idx = timevar.nearest_neighbour_index(time)\n",
    "    except IndexError:\n",
    "        idx = -1\n",
    "    return idx\n",
    "\n",
    "\n",
    "def time_slice(cube, start, stop=None):\n",
    "    \"\"\"Slice time by indexes using a nearest criteria.\n",
    "    NOTE: Assumes time is the first dimension!\"\"\"\n",
    "    istart = time_near(cube, start)\n",
    "    if stop:\n",
    "        istop = time_near(cube, stop)\n",
    "        if istart == istop:\n",
    "            raise ValueError('istart must be different from istop! '\n",
    "                             'Got istart {!r} and '\n",
    "                             ' istop {!r}'.format(istart, istop))\n",
    "        return cube[istart:istop, ...]\n",
    "    else:\n",
    "        return cube[istart, ...]\n",
    "\n",
    "\n",
    "def time_constraint(cube, start, stop):\n",
    "    \"\"\"Slice time by constraint.\"\"\"\n",
    "    begin = lambda cell: cell >= start\n",
    "    end = lambda cell: cell <= stop\n",
    "    constraint = Constraint(begin & end)\n",
    "    return cube.extract(constraint)\n",
    "\n",
    "\n",
    "def minmax(v):\n",
    "    return np.min(v), np.max(v)\n",
    "\n",
    "\n",
    "def bbox_extract_2Dcoords(cube, bbox):\n",
    "    \"\"\"Extract a sub-set of a cube inside a lon, lat bounding box\n",
    "    bbox=[lon_min lon_max lat_min lat_max].\n",
    "    NOTE: This is a work around too subset an iris cube that has\n",
    "    2D lon, lat coords.\"\"\"\n",
    "    lons = cube.coord('longitude').points\n",
    "    lats = cube.coord('latitude').points\n",
    "    lons = wrap_lon180(lons)\n",
    "\n",
    "    inregion = np.logical_and(np.logical_and(lons > bbox[0],\n",
    "                                             lons < bbox[2]),\n",
    "                              np.logical_and(lats > bbox[1],\n",
    "                                             lats < bbox[3]))\n",
    "    region_inds = np.where(inregion)\n",
    "    imin, imax = minmax(region_inds[0])\n",
    "    jmin, jmax = minmax(region_inds[1])\n",
    "    return cube[..., imin:imax+1, jmin:jmax+1]\n",
    "\n",
    "\n",
    "def bbox_extract_1Dcoords(cube, bbox):\n",
    "    lat = Constraint(latitude=lambda cell: bbox[1] <= cell < bbox[3])\n",
    "    lon = Constraint(longitude=lambda cell: bbox[0] <= cell <= bbox[2])\n",
    "    cube = cube.extract(lon & lat)\n",
    "    return cube\n",
    "\n",
    "\n",
    "def subset(cube, bbox):\n",
    "    \"\"\"Sub sets cube with 1D or 2D lon, lat coords.\n",
    "    Using `intersection` instead of `extract` we deal with 0--360\n",
    "    longitudes automagically.\"\"\"\n",
    "    if (cube.coord(axis='X').ndim == 1 and cube.coord(axis='Y').ndim == 1):\n",
    "        # Workaround `cube.intersection` hanging up on FVCOM models.\n",
    "        title = cube.attributes.get('title', None)\n",
    "        featureType = cube.attributes.get('featureType', None)\n",
    "        if (('FVCOM' in title) or ('ESTOFS' in title) or\n",
    "           featureType == 'timeSeries'):\n",
    "            cube = bbox_extract_1Dcoords(cube, bbox)\n",
    "        else:\n",
    "            cube = cube.intersection(longitude=(bbox[0], bbox[2]),\n",
    "                                     latitude=(bbox[1], bbox[3]))\n",
    "    elif (cube.coord(axis='X').ndim == 2 and\n",
    "          cube.coord(axis='Y').ndim == 2):\n",
    "        cube = bbox_extract_2Dcoords(cube, bbox)\n",
    "    else:\n",
    "        msg = \"Cannot deal with X:{!r} and Y:{!r} dimensions.\"\n",
    "        raise CoordinateMultiDimError(msg.format(cube.coord(axis='X').ndim),\n",
    "                                      cube.coord(axis='y').ndim)\n",
    "    return cube\n",
    "\n",
    "\n",
    "def get_cube(url, name_list, bbox=None, time=None, units=None, callback=None,\n",
    "             constraint=None):\n",
    "    \"\"\"Only `url` and `name_list` are mandatory.  The kw args are:\n",
    "    `bbox`, `callback`, `time`, `units`, `constraint`.\"\"\"\n",
    "\n",
    "    cubes = iris.load_raw(url, callback=callback)\n",
    "\n",
    "    in_list = lambda cube: cube.standard_name in name_list\n",
    "    cubes = CubeList([cube for cube in cubes if in_list(cube)])\n",
    "    if not cubes:\n",
    "        raise ValueError('Cube does not contain {!r}'.format(name_list))\n",
    "    else:\n",
    "        cube = cubes.merge_cube()\n",
    "\n",
    "    if constraint:\n",
    "        cube = cube.extract(constraint)\n",
    "        if not cube:\n",
    "            raise ValueError('No cube using {!r}'.format(constraint))\n",
    "    if bbox:\n",
    "        cube = subset(cube, bbox)\n",
    "        if not cube:\n",
    "            raise ValueError('No cube using {!r}'.format(bbox))\n",
    "    if time:\n",
    "        if isinstance(time, datetime):\n",
    "            start, stop = time, None\n",
    "        elif isinstance(time, tuple):\n",
    "            start, stop = time[0], time[1]\n",
    "        else:\n",
    "            raise ValueError('Time must be start or (start, stop).'\n",
    "                             '  Got {!r}'.format(time))\n",
    "        cube = time_slice(cube, start, stop)\n",
    "    if units:\n",
    "        if cube.units != units:\n",
    "            cube.convert_units(units)\n",
    "    return cube\n",
    "\n",
    "\n",
    "def remove_parenthesis(word):\n",
    "    try:\n",
    "        return word[word.index(\"(\") + 1:word.rindex(\")\")]\n",
    "    except ValueError:\n",
    "        return word\n",
    "\n",
    "\n",
    "def get_model_name(cube, url):\n",
    "    url = parse_url(url)\n",
    "    try:\n",
    "        model_full_name = cube.attributes['title']\n",
    "    except AttributeError:\n",
    "        model_full_name = url\n",
    "    words = []\n",
    "    for word in model_full_name.split():\n",
    "        if word.isupper():\n",
    "            words.append(remove_parenthesis(word))\n",
    "    mod_name = ' '.join(words)\n",
    "    if not mod_name:\n",
    "        mod_name = ''.join([c for c in model_full_name.split('(')[0]\n",
    "                            if c.isupper()])\n",
    "    if len(mod_name.split()) > 1:\n",
    "        mod_name = '_'.join(mod_name.split()[:2])\n",
    "    return mod_name, model_full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from iris.exceptions import ConstraintMismatchError, MergeError\n",
    "\n",
    "\n",
    "log.info(fmt(' Models '))\n",
    "cubes = dict()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # Suppress iris warnings :\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for k, url in enumerate(dap_urls):\n",
    "        log.info('\\n[Reading url {}/{}]: {}'.format(k+1, len(dap_urls), url))\n",
    "        try:\n",
    "            with time_limit(60*5):\n",
    "                cube = get_cube(url, name_list=name_list,\n",
    "                                bbox=bbox, time=(start, stop),\n",
    "                                units=iris.unit.Unit('meters'))\n",
    "            # TODO: Need a better way to identify model data and observed data.\n",
    "            if cube.ndim > 1:\n",
    "                mod_name, model_full_name = get_model_name(cube, url)\n",
    "                cubes.update({mod_name: cube})\n",
    "            else:\n",
    "                log.warning('url {} is probably a timeSeries!'.format(url))\n",
    "        except (RuntimeError, ValueError, MergeError, TimeoutException,\n",
    "                ConstraintMismatchError, CoordinateNotFoundError) as e:\n",
    "            log.warning('Cannot get cube for: {}\\n{}'.format(url, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "\n",
    "def standardize_fill_value(cube):\n",
    "    \"\"\"Work around default `fill_value` when obtaining\n",
    "    `_CubeSignature` (iris) using `lazy_data()` (biggus).\n",
    "    Warning use only when you DO KNOW that the slices should\n",
    "    have the same `fill_value`!!!\"\"\"\n",
    "    if ma.isMaskedArray(cube._my_data):\n",
    "        fill_value = ma.empty(0, dtype=cube._my_data.dtype).fill_value\n",
    "        cube._my_data.fill_value = fill_value\n",
    "    return cube\n",
    "\n",
    "\n",
    "def make_tree(cube):\n",
    "    \"\"\"Create KDTree.\"\"\"\n",
    "    lon = cube.coord(axis='X').points\n",
    "    lat = cube.coord(axis='Y').points\n",
    "    # Structured models with 1D lon, lat.\n",
    "    if (lon.ndim == 1) and (lat.ndim == 1) and (cube.ndim == 3):\n",
    "        lon, lat = np.meshgrid(lon, lat)\n",
    "    # Unstructure are already paired!\n",
    "    tree = KDTree(zip(lon.ravel(), lat.ravel()))\n",
    "    return tree, lon, lat\n",
    "\n",
    "\n",
    "def get_nearest_water(cube, tree, xi, yi, k=10, max_dist=0.04, min_var=0.01):\n",
    "    \"\"\"Find `k` nearest model data points from an iris `cube` at station\n",
    "    lon: `xi`, lat: `yi` up to `max_dist` in degrees.  Must provide a Scipy's\n",
    "    KDTree `tree`.\"\"\"\n",
    "    # TODO: Use rtree instead of KDTree.\n",
    "    # NOTE: Based on the iris `_nearest_neighbour_indices_ndcoords`.\n",
    "\n",
    "    distances, indices = tree.query(np.array([xi, yi]).T, k=k)\n",
    "    if indices.size == 0:\n",
    "        raise ValueError(\"No data found.\")\n",
    "    # Get data up to specified distance.\n",
    "    mask = distances <= max_dist\n",
    "    distances, indices = distances[mask], indices[mask]\n",
    "    if distances.size == 0:\n",
    "        msg = \"No data near ({}, {}) max_dist={}.\".format\n",
    "        raise ValueError(msg(xi, yi, max_dist))\n",
    "    # Unstructured model.\n",
    "    if (cube.coord(axis='X').ndim == 1) and (cube.ndim == 2):\n",
    "        i = j = indices\n",
    "        unstructured = True\n",
    "    # Structured model.\n",
    "    else:\n",
    "        unstructured = False\n",
    "        if cube.coord(axis='X').ndim == 2:  # CoordinateMultiDim\n",
    "            i, j = np.unravel_index(indices, cube.coord(axis='X').shape)\n",
    "        else:\n",
    "            shape = (cube.coord(axis='Y').shape[0],\n",
    "                     cube.coord(axis='X').shape[0])\n",
    "            i, j = np.unravel_index(indices, shape)\n",
    "    # Use only data where the standard deviation of the time series exceeds\n",
    "    # 0.01 m (1 cm) this eliminates flat line model time series that come from\n",
    "    # land points that should have had missing values.\n",
    "    series, dist, idx = None, None, None\n",
    "    for dist, idx in zip(distances, zip(i, j)):\n",
    "        if unstructured:  # NOTE: This would be so elegant in py3k!\n",
    "            idx = (idx[0],)\n",
    "        # This weird syntax allow for idx to be len 1 or 2.\n",
    "        series = cube[(slice(None),)+idx]\n",
    "        # Accounting for wet-and-dry models.\n",
    "        arr = ma.masked_invalid(series.data).filled(fill_value=0)\n",
    "        if arr.std() <= min_var:\n",
    "            series = None\n",
    "            break\n",
    "    return series, dist, idx\n",
    "\n",
    "\n",
    "def add_station(cube, station):\n",
    "    \"\"\"Add a station Auxiliary Coordinate and its name.\"\"\"\n",
    "    kw = dict(var_name=\"station\", long_name=\"station name\")\n",
    "    coord = iris.coords.AuxCoord(station, **kw)\n",
    "    cube.add_aux_coord(coord)\n",
    "    return cube\n",
    "\n",
    "\n",
    "def ensure_timeseries(cube):\n",
    "    \"\"\"Ensure that the cube is CF-timeSeries compliant.\"\"\"\n",
    "    if not cube.coord('time').shape == cube.shape[0]:\n",
    "        cube.transpose()\n",
    "    make_aux_coord(cube, axis='Y')\n",
    "    make_aux_coord(cube, axis='X')\n",
    "\n",
    "    cube.attributes.update({'featureType': 'timeSeries'})\n",
    "    cube.coord(\"station name\").attributes = dict(cf_role='timeseries_id')\n",
    "    return cube\n",
    "\n",
    "\n",
    "def make_aux_coord(cube, axis='Y'):\n",
    "    \"\"\"Make any given coordinate an Auxiliary Coordinate.\"\"\"\n",
    "    coord = cube.coord(axis=axis)\n",
    "    cube.remove_coord(coord)\n",
    "    if cube.ndim == 2:\n",
    "        cube.add_aux_coord(coord, 1)\n",
    "    else:\n",
    "        cube.add_aux_coord(coord)\n",
    "    return cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from iris.pandas import as_series\n",
    "\n",
    "\n",
    "for mod_name, cube in cubes.items():\n",
    "    fname = '{}-{}.nc'.format(run_name, mod_name)\n",
    "    log.info(fmt(' Saving to file {} '.format(fname)))\n",
    "    # NOTE: 2D coords KDtree.  (Iris can only do 1D coords KDtree for now.)\n",
    "    try:\n",
    "        tree, lon, lat = make_tree(cube)\n",
    "    except CoordinateNotFoundError as e:\n",
    "        log.warning('Cannot create KDTree for: {}'.format(mod_name))\n",
    "        continue\n",
    "    # Get model series at observed locations.\n",
    "    raw_series = dict()\n",
    "    for station, obs in observations.iterrows():\n",
    "        try:\n",
    "            kw = dict(k=10, max_dist=0.04, min_var=0.01)\n",
    "            args = cube, tree, obs.lon, obs.lat\n",
    "            series, dist, idx = get_nearest_water(*args, **kw)\n",
    "        except ValueError as e:\n",
    "            log.warning(e)\n",
    "            continue\n",
    "        if not series:\n",
    "            status = \"Land \"\n",
    "        else:\n",
    "            raw_series.update({obs['station']: series})\n",
    "            series = as_series(series)\n",
    "            status = \"Water\"\n",
    "\n",
    "        log.info('[{}] {}'.format(status, obs.name))\n",
    "\n",
    "    if raw_series:  # Save cube.\n",
    "        for station, cube in raw_series.items():\n",
    "            cube = standardize_fill_value(cube)\n",
    "            cube = add_station(cube, station)\n",
    "        try:\n",
    "            cube = iris.cube.CubeList(raw_series.values()).merge_cube()\n",
    "        except MergeError as e:\n",
    "            log.warning(e)\n",
    "\n",
    "        ensure_timeseries(cube)\n",
    "        iris.save(cube, fname)\n",
    "        del cube\n",
    "\n",
    "    log.info('Finished processing [{}]: {}'.format(mod_name, url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add extra stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include = dict({'Scituate, MA': dict(lon=-70.7166, lat=42.9259),\n",
    "                'Wells, ME': dict(lon=-70.583883, lat=43.272411)})\n",
    "\n",
    "models = dict()\n",
    "extra_series = dict()\n",
    "for station, obs in include.items():\n",
    "    for mod_name, cube in cubes.items():\n",
    "        mod_name, model_full_name = get_model_name(cube, url)\n",
    "        try:\n",
    "            tree, lon, lat = make_tree(cube)\n",
    "        except CoordinateNotFoundError as e:\n",
    "            log.warning('Cannot create KDTree for: {}'.format(mod_name))\n",
    "            continue\n",
    "        # Get model series at observed locations.\n",
    "        try:\n",
    "            kw = dict(k=10, max_dist=0.04, min_var=0.01)\n",
    "            args = cube, tree, obs['lon'], obs['lat']\n",
    "            series, dist, idx = get_nearest_water(*args, **kw)\n",
    "        except ValueError as e:\n",
    "            log.warning(e)\n",
    "            continue\n",
    "        if not series:\n",
    "            status = \"Land \"\n",
    "        else:\n",
    "            status = \"Water\"\n",
    "            models.update({mod_name: series})\n",
    "\n",
    "        log.info('[{}] {}'.format(status, station))\n",
    "    extra_series.update({station: models})\n",
    "    models = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved files and interpolate to the observations time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from iris.pandas import as_data_frame\n",
    "\n",
    "\n",
    "def nc2df(fname):\n",
    "    cube = iris.load_cube(fname)\n",
    "    for coord in cube.coords(dimensions=[0]):\n",
    "        name = coord.name()\n",
    "        if name != 'time':\n",
    "            cube.remove_coord(name)\n",
    "    for coord in cube.coords(dimensions=[1]):\n",
    "        name = coord.name()\n",
    "        if name != 'station name':\n",
    "            cube.remove_coord(name)\n",
    "    df = as_data_frame(cube)\n",
    "    if cube.ndim == 1:  # Horrible work around iris.\n",
    "        station = cube.coord('station name').points[0]\n",
    "        df.columns = [station]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from operator import itemgetter\n",
    "\n",
    "from pandas import Panel\n",
    "\n",
    "fname = '{}-OBS_DATA.nc'.format(run_name)\n",
    "OBS_DATA = nc2df(fname)\n",
    "OBS_DATA.index = OBS_DATA.index.tz_localize(start.tzinfo)\n",
    "\n",
    "from pandas import date_range\n",
    "index = date_range(start=start, end=stop, freq='6min', tz=start.tzinfo)\n",
    "\n",
    "dfs = dict(OBS_DATA=OBS_DATA)\n",
    "for fname in glob(\"*.nc\"):\n",
    "    if 'OBS_DATA' in fname:\n",
    "        continue\n",
    "    else:\n",
    "        model = fname.split('.')[0].split('-')[-1]\n",
    "        df = nc2df(fname)\n",
    "        if len(df.index.values) != len(np.unique(df.index.values)):\n",
    "            # FIXME: Horrible work around duplicate times.\n",
    "            opts = dict(cols='index', take_last=True)\n",
    "            df = df.reset_index().drop_duplicates(**opts).set_index('index')\n",
    "        df.index = df.index.tz_localize(start.tzinfo)\n",
    "        if False:  # if True interpolates to 6 min series.\n",
    "            kw = dict(method='time', limit=30)\n",
    "            df = df.reindex(index).interpolate(**kw).ix[index]\n",
    "        dfs.update({model: df})\n",
    "\n",
    "dfs = Panel.fromDict(dfs).swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COAWST_USGS</th>\n",
       "      <th>ESTOFS_NOAA</th>\n",
       "      <th>NECOFS_FVCOM</th>\n",
       "      <th>NECOFS_GOM3</th>\n",
       "      <th>ROMS_ESPRESSO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Portland, ME</th>\n",
       "      <td>   --</td>\n",
       "      <td>    --</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.21</td>\n",
       "      <td>    --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fort Point, NH</th>\n",
       "      <td>   --</td>\n",
       "      <td>  0.01</td>\n",
       "      <td> 0.21</td>\n",
       "      <td> 0.19</td>\n",
       "      <td>    --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston, MA</th>\n",
       "      <td>   --</td>\n",
       "      <td>    --</td>\n",
       "      <td> 0.23</td>\n",
       "      <td> 0.17</td>\n",
       "      <td>    --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woods Hole, MA</th>\n",
       "      <td> 0.07</td>\n",
       "      <td> -0.09</td>\n",
       "      <td> 0.20</td>\n",
       "      <td> 0.21</td>\n",
       "      <td>    --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Newport, RI</th>\n",
       "      <td> 0.09</td>\n",
       "      <td> -0.13</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.27</td>\n",
       "      <td> -0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Providence, RI</th>\n",
       "      <td>   --</td>\n",
       "      <td>    --</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.31</td>\n",
       "      <td>    --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montauk, NY</th>\n",
       "      <td>   --</td>\n",
       "      <td> -0.15</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.23</td>\n",
       "      <td>    --</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               COAWST_USGS ESTOFS_NOAA NECOFS_FVCOM NECOFS_GOM3 ROMS_ESPRESSO\n",
       "Portland, ME            --          --           --        0.21            --\n",
       "Fort Point, NH          --        0.01         0.21        0.19            --\n",
       "Boston, MA              --          --         0.23        0.17            --\n",
       "Woods Hole, MA        0.07       -0.09         0.20        0.21            --\n",
       "Newport, RI           0.09       -0.13           --        0.27         -0.42\n",
       "Providence, RI          --          --           --        0.31            --\n",
       "Montauk, NY             --       -0.15           --        0.23            --"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "panel = dfs.copy()\n",
    "means = dict()\n",
    "for station, df in panel.iteritems():\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    mean = df.mean()\n",
    "    df = df - mean + mean['OBS_DATA']\n",
    "    means.update({station: mean.drop('OBS_DATA') - mean['OBS_DATA']})\n",
    "\n",
    "bias = DataFrame.from_dict(means).dropna(axis=1, how='all')\n",
    "bias = bias.applymap('{:.2f}'.format).replace('nan', '--')\n",
    "\n",
    "columns = dict()\n",
    "[columns.update({station: get_coops_longname(station)}) for\n",
    " station in bias.columns.values]\n",
    "\n",
    "bias.rename(columns=columns, inplace=True)\n",
    "\n",
    "bias.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def both_valid(x, y):\n",
    "    \"\"\"Returns a mask where both series are valid.\"\"\"\n",
    "    mask_x = np.isnan(x)\n",
    "    mask_y = np.isnan(y)\n",
    "    return np.logical_and(~mask_x, ~mask_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "skills = dict()\n",
    "for station, df in panel.iteritems():\n",
    "    obs = df['OBS_DATA']\n",
    "    skill = dict()\n",
    "    for model, y in df.iteritems():\n",
    "        if 'OBS_DATA' not in model:\n",
    "            mask = both_valid(obs, y)\n",
    "            r, p = pearsonr(obs[mask]-obs.mean(), y[mask]-y.mean())\n",
    "            skill.update({model: r})\n",
    "    skills.update({station: skill})\n",
    "\n",
    "df = DataFrame.from_dict(skills)\n",
    "\n",
    "columns = dict()\n",
    "[columns.update({station: get_coops_longname(station)}) for\n",
    " station in df.columns.values]\n",
    "\n",
    "df.rename(columns=columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COAWST_USGS</th>\n",
       "      <th>ESTOFS_NOAA</th>\n",
       "      <th>NECOFS_FVCOM</th>\n",
       "      <th>NECOFS_GOM3</th>\n",
       "      <th>ROMS_ESPRESSO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Portland, ME</th>\n",
       "      <td>   --</td>\n",
       "      <td>   --</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.98</td>\n",
       "      <td>   --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fort Point, NH</th>\n",
       "      <td>   --</td>\n",
       "      <td> 0.98</td>\n",
       "      <td> 0.99</td>\n",
       "      <td> 0.98</td>\n",
       "      <td>   --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston, MA</th>\n",
       "      <td>   --</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.98</td>\n",
       "      <td> 0.98</td>\n",
       "      <td>   --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woods Hole, MA</th>\n",
       "      <td> 0.80</td>\n",
       "      <td> 0.79</td>\n",
       "      <td> 0.80</td>\n",
       "      <td> 0.74</td>\n",
       "      <td>   --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Newport, RI</th>\n",
       "      <td> 0.85</td>\n",
       "      <td> 0.86</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.97</td>\n",
       "      <td> 0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Providence, RI</th>\n",
       "      <td>   --</td>\n",
       "      <td>   --</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.94</td>\n",
       "      <td>   --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montauk, NY</th>\n",
       "      <td>   --</td>\n",
       "      <td> 0.83</td>\n",
       "      <td>   --</td>\n",
       "      <td> 0.95</td>\n",
       "      <td>   --</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               COAWST_USGS ESTOFS_NOAA NECOFS_FVCOM NECOFS_GOM3 ROMS_ESPRESSO\n",
       "Portland, ME            --          --           --        0.98            --\n",
       "Fort Point, NH          --        0.98         0.99        0.98            --\n",
       "Boston, MA              --          --         0.98        0.98            --\n",
       "Woods Hole, MA        0.80        0.79         0.80        0.74            --\n",
       "Newport, RI           0.85        0.86           --        0.97          0.73\n",
       "Providence, RI          --          --           --        0.94            --\n",
       "Montauk, NY             --        0.83           --        0.95            --"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'skill.html'\n",
    "\n",
    "df_skill = df.applymap('{:.2f}'.format).replace('nan', '--')\n",
    "\n",
    "df_skill.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from folium.folium import Map\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "def get_coordinates(bbox):\n",
    "    \"\"\"Create bounding box coordinates for the map.  It takes flat or\n",
    "    nested list/numpy.array and returns 4 points for the map corners.\"\"\"\n",
    "    bbox = np.asanyarray(bbox).ravel()\n",
    "    if bbox.size == 4:\n",
    "        bbox = bbox.reshape(2, 2)\n",
    "        coordinates = []\n",
    "        coordinates.append([bbox[0][1], bbox[0][0]])\n",
    "        coordinates.append([bbox[0][1], bbox[1][0]])\n",
    "        coordinates.append([bbox[1][1], bbox[1][0]])\n",
    "        coordinates.append([bbox[1][1], bbox[0][0]])\n",
    "        coordinates.append([bbox[0][1], bbox[0][0]])\n",
    "    else:\n",
    "        raise ValueError('Wrong number corners.'\n",
    "                         '  Expected 4 got {}'.format(bbox.size))\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def inline_map(m):\n",
    "    \"\"\"Takes a folium instance or a html path and load into an iframe.\"\"\"\n",
    "    if isinstance(m, Map):\n",
    "        m._build_map()\n",
    "        srcdoc = m.HTML.replace('\"', '&quot;')\n",
    "        embed = HTML('<iframe srcdoc=\"{srcdoc}\" '\n",
    "                     'style=\"width: 100%; height: 500px; '\n",
    "                     'border: none\"></iframe>'.format(srcdoc=srcdoc))\n",
    "    elif isinstance(m, str):\n",
    "        embed = IFrame(m, width=750, height=500)\n",
    "    return embed\n",
    "\n",
    "\n",
    "def make_map(bbox, **kw):\n",
    "    \"\"\"Creates a folium map instance.\"\"\"\n",
    "    line = kw.pop('line', True)\n",
    "    zoom_start = kw.pop('zoom_start', 7)\n",
    "\n",
    "    lon, lat = np.array(bbox).reshape(2, 2).mean(axis=0)\n",
    "    m = Map(width=750, height=500,\n",
    "            location=[lat, lon], zoom_start=zoom_start)\n",
    "    if line:\n",
    "        # Create the map and add the bounding box line.\n",
    "        kw = dict(line_color='#FF0000', line_weight=2)\n",
    "        m.line(get_coordinates(bbox), **kw)\n",
    "    return m\n",
    "\n",
    "\n",
    "def plot_series():\n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "    ax.set_ylabel('Sea surface height (m)')\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.grid(True)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clusters.\n",
    "big_list = []\n",
    "for fname in glob(\"*.nc\"):\n",
    "    if 'OBS_DATA' in fname:\n",
    "        continue\n",
    "    nc = iris.load_cube(fname)\n",
    "    model = fname.split('-')[-1].split('.')[0]\n",
    "    lons = nc.coord(axis='X').points\n",
    "    lats = nc.coord(axis='Y').points\n",
    "    stations = nc.coord('station name').points\n",
    "    models = [model]*lons.size\n",
    "    lista = zip(models, lons.tolist(), lats.tolist(), stations.tolist())\n",
    "    big_list.extend(lista)\n",
    "\n",
    "big_list.sort(key=itemgetter(3))\n",
    "df = DataFrame(big_list, columns=['name', 'lon', 'lat', 'station'])\n",
    "df.set_index('station', drop=True, inplace=True)\n",
    "groups = df.groupby(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpld3 import save_html\n",
    "from mpld3.plugins import LineLabelTooltip, connect\n",
    "\n",
    "mapa = make_map(bbox, line=True, states=False)\n",
    "\n",
    "# Clusters.\n",
    "for station, info in groups:\n",
    "    station = get_coops_longname(station)\n",
    "    for lat, lon, name in zip(info.lat, info.lon, info.name):\n",
    "        location = lat, lon\n",
    "        popup = '<b>{}</b>\\n{}'.format(station, name)\n",
    "        mapa.simple_marker(location=location, popup=popup,\n",
    "                           clustered_marker=True)\n",
    "\n",
    "# Model and observations.\n",
    "resolution, width, height = 75, 7, 3\n",
    "for station in dfs:\n",
    "    sta_name = get_coops_longname(station)\n",
    "    # This will eliminate all NaNs columns.\n",
    "    df = dfs[station].dropna(axis=1, how='all')\n",
    "\n",
    "    fig, ax = plot_series()\n",
    "    labels = []\n",
    "    for col in df.columns:\n",
    "        # This restore the series to its original \"index.\"\n",
    "        # Not needed if interpolating the series.\n",
    "        serie = df[col].dropna()\n",
    "        lines = ax.plot(serie.index, serie, label=col,\n",
    "                        linewidth=2.5, alpha=0.5)\n",
    "        if 'OBS_DATA' not in col:\n",
    "            text0 = col\n",
    "            text1 = bias[sta_name][col]\n",
    "            text2 = df_skill[sta_name][col]\n",
    "            tooltip = '{}:\\nbias {}\\nskill: {}'.format\n",
    "            labels.append(tooltip(text0, text1, text2))\n",
    "        else:\n",
    "            labels.append('OBS_DATA')\n",
    "\n",
    "    kw = dict(loc='upper center', bbox_to_anchor=(0.5, 1.05), numpoints=1,\n",
    "              ncol=2, framealpha=0)\n",
    "    l = ax.legend(**kw)\n",
    "    l.set_title(\"\")  # Workaround str(None).\n",
    "\n",
    "    [connect(fig, LineLabelTooltip(line, name))\n",
    "     for line, name in zip(ax.lines, labels)]\n",
    "\n",
    "    html = 'station_{}.html'.format(station)\n",
    "    save_html(fig, '{}'.format(html))\n",
    "\n",
    "    popup = \"<div align='center'> {} <br><iframe src='{}' alt='image'\"\n",
    "    popup += \"width='{}px' height='{}px' frameBorder='0'></div>\"\n",
    "    popup = popup.format('{}'.format(sta_name), html,\n",
    "                         (width*resolution)+75, (height*resolution)+50)\n",
    "    kw = dict(popup=popup, width=(width*resolution)+75)\n",
    "\n",
    "    if (df.columns == 'OBS_DATA').all():\n",
    "        kw.update(dict(marker_color=\"blue\", marker_icon=\"ok\"))\n",
    "    else:\n",
    "        kw.update(dict(marker_color=\"green\", marker_icon=\"ok-sign\"))\n",
    "    obs = observations[observations['station'] == station].squeeze()\n",
    "    mapa.simple_marker(location=[obs['lat'], obs['lon']], **kw)\n",
    "\n",
    "# Bad datum.\n",
    "if isinstance(bad_datum, DataFrame):\n",
    "    for station, obs in bad_datum.iterrows():\n",
    "        popup = '<b>Station:</b> {}<br><b>Datum:</b> {}<br>'\n",
    "        popup = popup.format(station, obs['datum'])\n",
    "        kw = dict(popup=popup, marker_color=\"red\", marker_icon=\"question-sign\")\n",
    "        mapa.simple_marker(location=[obs['lat'], obs['lon']], **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for station in include.keys():\n",
    "    models = extra_series[station]\n",
    "    if models:\n",
    "        fig, ax = plot_series()\n",
    "        labels = []\n",
    "        for model, cube in models.items():\n",
    "            t = time_coord(cube)\n",
    "            t = t.units.num2date(t.points)\n",
    "            lines = ax.plot(t, cube.data, linewidth=2.5, alpha=0.5,\n",
    "                            label=model)\n",
    "            labels.append(model)\n",
    "\n",
    "        kw = dict(loc='upper center', bbox_to_anchor=(0.5, 1.05), numpoints=1,\n",
    "                  ncol=2, framealpha=0)\n",
    "        l = ax.legend(**kw)\n",
    "        l.set_title(\"\")  # Workaround str(None).\n",
    "\n",
    "        [connect(fig, LineLabelTooltip(line, name))\n",
    "         for line, name in zip(ax.lines, labels)]\n",
    "\n",
    "        html = 'station_{}.html'.format\n",
    "        html = html(station.lower().replace(' ', '_').replace(',', ''))\n",
    "        save_html(fig, '{}'.format(html))\n",
    "\n",
    "        popup = \"<div align='center'> {} <br><iframe src='{}' alt='image'\"\n",
    "        popup += \"width='{}px' height='{}px' frameBorder='0'></div>\"\n",
    "        popup = popup.format('{}'.format(station), html,\n",
    "                             (width*resolution)+75, (height*resolution)+50)\n",
    "        kw = dict(popup=popup, width=(width*resolution)+75)\n",
    "\n",
    "        kw.update(dict(marker_color=\"green\", marker_icon=\"ok\"))\n",
    "        obs = observations[observations['station'] == station].squeeze()\n",
    "        mapa.simple_marker(location=[include[station]['lat'],\n",
    "                                     include[station]['lon']], **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"750\"\n",
       "            height=500\"\n",
       "            src=\"mapa.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f72989b2050>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapa.create_map(path='mapa.html')\n",
    "inline_map('mapa.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elapsed = time.time() - start_time\n",
    "log.info('{:.2f} minutes'.format(elapsed/60.))\n",
    "log.info('EOF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:54:57 INFO: *********************** Run information ************************\n",
      "03:54:57 INFO: Run date: 2015-01-28 18:54:57\n",
      "03:54:57 INFO: Download start: 2015-01-24 00:00:00\n",
      "03:54:57 INFO: Download stop: 2015-01-31 00:00:00\n",
      "03:54:57 INFO: Bounding box: -72.00, 41.00,-69.00, 44.00\n",
      "03:54:57 INFO: *********************** Software version ***********************\n",
      "03:54:57 INFO: Iris version: 1.7.2-DEV\n",
      "03:54:57 INFO: owslib version: 0.8-dev\n",
      "03:54:57 INFO: pyoos version: 0.6.2\n",
      "03:55:00 INFO: ********************* Catalog information **********************\n",
      "03:55:00 INFO: URL: http://www.ngdc.noaa.gov/geoportal/csw\n",
      "03:55:00 INFO: CSW version: 2.0.2\n",
      "03:55:00 INFO: Number of datasets available: 7\n",
      "03:55:01 INFO: ***************************** CSW ******************************\n",
      "03:55:01 INFO: NECOFS Massachusetts (FVCOM) - Massachusetts Coastal - Latest Forecast\n",
      "03:55:01 INFO: NECOFS GOM3 (FVCOM) - Northeast US - Latest Forecast\n",
      "03:55:01 INFO: ROMS ESPRESSO Real-Time Operational IS4DVAR Forecast System Version 2 (NEW) 2013-present FMRC History (Best)\n",
      "03:55:01 INFO: COAWST Forecast System : USGS : US East Coast and Gulf of Mexico (Experimental)\n",
      "03:55:01 INFO: Barotropic Tide Model for the Pacific Basin\n",
      "03:55:01 INFO: ESTOFS Storm Surge Model - Atlantic - v1.0.0 - NOAA - NCEP - ADCIRC\n",
      "03:55:01 INFO: HYbrid Coordinate Ocean Model (HYCOM): Global\n",
      "03:55:01 INFO: ***************************** DAP ******************************\n",
      "03:55:01 INFO: http://geoport-dev.whoi.edu/thredds/dodsC/estofs/atlantic.html\n",
      "03:55:01 INFO: http://geoport.whoi.edu/thredds/dodsC/coawst_4/use/fmrc/coawst_4_use_best.ncd.html\n",
      "03:55:01 INFO: http://oos.soest.hawaii.edu/thredds/dodsC/hioos/tide_pac.html\n",
      "03:55:01 INFO: http://oos.soest.hawaii.edu/thredds/dodsC/pacioos/hycom/global.html\n",
      "03:55:01 INFO: http://tds.marine.rutgers.edu/thredds/dodsC/roms/espresso/2013_da/his_Best/ESPRESSO_Real-Time_v2_History_Best_Available_best.ncd.html\n",
      "03:55:01 INFO: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_FVCOM_OCEAN_MASSBAY_FORECAST.nc.html\n",
      "03:55:01 INFO: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc.html\n",
      "03:55:01 INFO: ***************************** SOS ******************************\n",
      "03:55:12 INFO: ********************* Collector offerings **********************\n",
      "03:55:12 INFO: NOAA.NOS.CO-OPS SOS: 1030 offerings\n",
      "03:55:13 INFO: Starting new HTTP connection (1): opendap.co-ops.nos.noaa.gov\n",
      "03:55:27 INFO: SOS URL request: http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?eventTime=2015-01-24T00%3A00%3A00Z&service=SOS&offering=urn%3Aioos%3Anetwork%3ANOAA.NOS.CO-OPS%3AWaterLevelActive&request=GetObservation&version=1.0.0&responseFormat=text%2Fcsv&featureOfInterest=BBOX%3A-72%2C41%2C-69%2C44&observedProperty=water_surface_height_above_reference_datum\n",
      "03:55:54 INFO: ************************* Observations *************************\n",
      "03:55:54 INFO: ********** Downloading to file 2015-01-28-OBS_DATA.nc **********\n",
      "03:56:08 WARNING: [8419317] Wells, ME:\n",
      "'Wrong Datum for this station: The correct Datum values are: MHHW, MHW, MTL, MSL, MLW, MLLW, STND'\n",
      "03:56:26 WARNING: [8447386] Fall River, MA:\n",
      "'Wrong Datum for this station: The correct Datum values are: MHHW, MHW, MTL, MSL, MLW, MLLW, STND'\n",
      "03:56:45 WARNING: [8449130] Nantucket Island, MA:\n",
      "'Wrong Datum for this station: The correct Datum values are: MHHW, MHW, MTL, MSL, MLW, MLLW, STND'\n",
      "03:57:04 WARNING: [8452944] Conimicut Light, RI:\n",
      "'Wrong Datum for this station: The correct Datum values are: MHHW, MHW, MTL, MSL, MLW, MLLW, STND'\n",
      "03:57:24 INFO: **************************** Models ****************************\n",
      "03:57:24 INFO: \n",
      "[Reading url 1/7]: http://geoport-dev.whoi.edu/thredds/dodsC/estofs/atlantic\n",
      "03:59:22 INFO: \n",
      "[Reading url 2/7]: http://geoport.whoi.edu/thredds/dodsC/coawst_4/use/fmrc/coawst_4_use_best.ncd\n",
      "04:02:17 INFO: \n",
      "[Reading url 3/7]: http://oos.soest.hawaii.edu/thredds/dodsC/hioos/tide_pac\n",
      "04:02:30 INFO: \n",
      "[Reading url 4/7]: http://oos.soest.hawaii.edu/thredds/dodsC/pacioos/hycom/global\n",
      "04:02:34 INFO: \n",
      "[Reading url 5/7]: http://tds.marine.rutgers.edu/thredds/dodsC/roms/espresso/2013_da/his_Best/ESPRESSO_Real-Time_v2_History_Best_Available_best.ncd\n",
      "04:02:47 INFO: \n",
      "[Reading url 6/7]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_FVCOM_OCEAN_MASSBAY_FORECAST.nc\n",
      "04:03:04 INFO: \n",
      "[Reading url 7/7]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:03:16 INFO: *********** Saving to file 2015-01-28-COAWST_USGS.nc ***********\n",
      "04:03:26 INFO: [Land ] Portland, ME\n",
      "04:03:35 INFO: [Land ] Fort Point, NH\n",
      "04:03:41 INFO: [Land ] Boston, MA\n",
      "04:03:46 INFO: [Water] Woods Hole, MA\n",
      "04:03:58 INFO: [Water] Newport, RI\n",
      "04:04:04 INFO: [Land ] Providence, RI\n",
      "04:04:17 INFO: [Land ] Montauk, NY\n",
      "04:04:26 INFO: Finished processing [COAWST_USGS]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:04:26 INFO: ************** Saving to file 2015-01-28-BTMPB.nc **************\n",
      "04:04:26 WARNING: No data near (-70.2467, 43.6567) max_dist=0.04.\n",
      "04:04:26 WARNING: No data near (-70.7117, 43.0717) max_dist=0.04.\n",
      "04:04:26 WARNING: No data near (-71.0534, 42.3548) max_dist=0.04.\n",
      "04:04:26 WARNING: No data near (-70.6717, 41.5233) max_dist=0.04.\n",
      "04:04:26 WARNING: No data near (-71.3267, 41.505) max_dist=0.04.\n",
      "04:04:26 WARNING: No data near (-71.4012, 41.8071) max_dist=0.04.\n",
      "04:04:26 WARNING: No data near (-71.96, 41.0483) max_dist=0.04.\n",
      "04:04:26 INFO: Finished processing [BTMPB]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:04:26 INFO: ************** Saving to file 2015-01-28-HYCOM.nc **************\n",
      "04:04:30 INFO: [Land ] Portland, ME\n",
      "04:04:32 INFO: [Land ] Fort Point, NH\n",
      "04:04:35 INFO: [Land ] Boston, MA\n",
      "04:04:35 WARNING: No data near (-70.6717, 41.5233) max_dist=0.04.\n",
      "04:04:38 INFO: [Land ] Newport, RI\n",
      "04:04:38 WARNING: No data near (-71.4012, 41.8071) max_dist=0.04.\n",
      "04:04:38 WARNING: No data near (-71.96, 41.0483) max_dist=0.04.\n",
      "04:04:38 INFO: Finished processing [HYCOM]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:04:38 INFO: *********** Saving to file 2015-01-28-ESTOFS_NOAA.nc ***********\n",
      "04:04:38 WARNING: No data near (-70.2467, 43.6567) max_dist=0.04.\n",
      "04:04:44 INFO: [Water] Fort Point, NH\n",
      "04:04:44 WARNING: No data near (-71.0534, 42.3548) max_dist=0.04.\n",
      "04:04:56 INFO: [Water] Woods Hole, MA\n",
      "04:05:07 INFO: [Water] Newport, RI\n",
      "04:05:07 WARNING: No data near (-71.4012, 41.8071) max_dist=0.04.\n",
      "04:05:18 INFO: [Water] Montauk, NY\n",
      "04:05:18 INFO: Finished processing [ESTOFS_NOAA]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:05:18 INFO: *********** Saving to file 2015-01-28-NECOFS_GOM3.nc ***********\n",
      "04:05:32 INFO: [Water] Portland, ME\n",
      "04:05:47 INFO: [Water] Fort Point, NH\n",
      "04:06:02 INFO: [Water] Boston, MA\n",
      "04:06:17 INFO: [Water] Woods Hole, MA\n",
      "04:06:32 INFO: [Water] Newport, RI\n",
      "04:06:46 INFO: [Water] Providence, RI\n",
      "04:07:01 INFO: [Water] Montauk, NY\n",
      "04:07:01 INFO: Finished processing [NECOFS_GOM3]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:07:01 INFO: ********** Saving to file 2015-01-28-NECOFS_FVCOM.nc ***********\n",
      "04:07:01 WARNING: No data near (-70.2467, 43.6567) max_dist=0.04.\n",
      "04:07:15 INFO: [Water] Fort Point, NH\n",
      "04:07:30 INFO: [Water] Boston, MA\n",
      "04:07:44 INFO: [Water] Woods Hole, MA\n",
      "04:07:44 WARNING: No data near (-71.3267, 41.505) max_dist=0.04.\n",
      "04:07:44 WARNING: No data near (-71.4012, 41.8071) max_dist=0.04.\n",
      "04:07:44 WARNING: No data near (-71.96, 41.0483) max_dist=0.04.\n",
      "04:07:44 INFO: Finished processing [NECOFS_FVCOM]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:07:44 INFO: ********** Saving to file 2015-01-28-ROMS_ESPRESSO.nc **********\n",
      "04:07:44 WARNING: No data near (-70.2467, 43.6567) max_dist=0.04.\n",
      "04:07:44 WARNING: No data near (-70.7117, 43.0717) max_dist=0.04.\n",
      "04:07:49 INFO: [Land ] Boston, MA\n",
      "04:07:53 INFO: [Land ] Woods Hole, MA\n",
      "04:07:57 INFO: [Water] Newport, RI\n",
      "04:08:01 INFO: [Land ] Providence, RI\n",
      "04:08:04 INFO: [Land ] Montauk, NY\n",
      "04:08:07 INFO: Finished processing [ROMS_ESPRESSO]: http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_GOM3_FORECAST.nc\n",
      "04:08:20 INFO: [Water] Scituate, MA\n",
      "04:08:20 WARNING: No data near (-70.7166, 42.9259) max_dist=0.04.\n",
      "04:08:23 INFO: [Water] Scituate, MA\n",
      "04:08:34 INFO: [Water] Scituate, MA\n",
      "04:08:48 INFO: [Water] Scituate, MA\n",
      "04:09:03 INFO: [Water] Scituate, MA\n",
      "04:09:03 WARNING: No data near (-70.7166, 42.9259) max_dist=0.04.\n",
      "04:09:09 INFO: [Land ] Wells, ME\n",
      "04:09:09 WARNING: No data near (-70.583883, 43.272411) max_dist=0.04.\n",
      "04:09:12 INFO: [Water] Wells, ME\n",
      "04:09:24 INFO: [Water] Wells, ME\n",
      "04:09:35 INFO: [Water] Wells, ME\n",
      "04:09:47 INFO: [Water] Wells, ME\n",
      "04:09:47 WARNING: No data near (-70.583883, 43.272411) max_dist=0.04.\n",
      "04:10:16 INFO: 15.35 minutes\n",
      "04:10:16 INFO: EOF\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('log.txt') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
